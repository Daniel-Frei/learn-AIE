import { Question } from "../../quiz";

export const cs224rLecture3PolicyGradientsQuestions: Question[] = [
  // ============================================================
  // Lecture 3 – Policy Gradients
  // Q1–Q35
  // ============================================================

  // ============================================================
  // Q1–Q9: ALL TRUE
  // ============================================================

  {
    id: "cs224r-lect3-q01",
    chapter: 3,
    difficulty: "easy",
    prompt:
      "Which statements correctly describe the reinforcement learning objective?",
    options: [
      {
        text: "The objective is to maximize expected cumulative reward.",
        isCorrect: true,
      },
      {
        text: "The expectation is taken over trajectories induced by the policy.",
        isCorrect: true,
      },
      {
        text: "The trajectory distribution depends on both the policy and environment dynamics.",
        isCorrect: true,
      },
      {
        text: "The objective can be written as \\( J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[\\sum_t r(s_t, a_t)] \\).",
        isCorrect: true,
      },
    ],
    explanation:
      "Reinforcement learning optimizes the expected sum of rewards over trajectories generated by a policy interacting with an environment. The expectation explicitly depends on the policy through the trajectory distribution.",
  },

  {
    id: "cs224r-lect3-q02",
    chapter: 3,
    difficulty: "easy",
    prompt:
      "Which statements correctly describe online reinforcement learning?",
    options: [
      {
        text: "The policy is updated using data collected from its own execution.",
        isCorrect: true,
      },
      {
        text: "Data collection and learning are interleaved.",
        isCorrect: true,
      },
      { text: "The policy improves through trial and error.", isCorrect: true },
      {
        text: "The data distribution changes as the policy changes.",
        isCorrect: true,
      },
    ],
    explanation:
      "Online reinforcement learning alternates between sampling trajectories and updating the policy. Because the policy changes, the data distribution is non-stationary.",
  },

  {
    id: "cs224r-lect3-q03",
    chapter: 3,
    difficulty: "easy",
    prompt:
      "Which statements correctly describe a policy \\(\\pi_\\theta(a\\mid s)\\)?",
    options: [
      {
        text: "It defines a probability distribution over actions.",
        isCorrect: true,
      },
      {
        text: "It is typically parameterized by a neural network.",
        isCorrect: true,
      },
      { text: "It induces a distribution over trajectories.", isCorrect: true },
      {
        text: "Its parameters are optimized to increase expected reward.",
        isCorrect: true,
      },
    ],
    explanation:
      "A policy maps states to action distributions. Through interaction with the environment, it induces a trajectory distribution whose expected reward is optimized.",
  },

  {
    id: "cs224r-lect3-q04",
    chapter: 3,
    difficulty: "medium",
    prompt:
      "Which statements correctly describe the probability of a trajectory \\(p_\\theta(\\tau)\\)?",
    options: [
      {
        text: "\\(p_\\theta(\\tau) = p(s_1) \\prod_t \\pi_\\theta(a_t\\mid s_t) p(s_{t+1}\\mid s_t, a_t)\\).",
        isCorrect: true,
      },
      {
        text: "It factorizes into policy and environment dynamics terms.",
        isCorrect: true,
      },
      {
        text: "The initial state distribution does not depend on \\(\\theta\\).",
        isCorrect: true,
      },
      {
        text: "Environment transition probabilities are independent of \\(\\theta\\).",
        isCorrect: true,
      },
    ],
    explanation:
      "The trajectory distribution factorizes into the initial state, policy, and environment dynamics. Only the policy term depends on the parameters \\(\\theta\\).",
  },

  {
    id: "cs224r-lect3-q05",
    chapter: 3,
    difficulty: "medium",
    prompt:
      "Which statements correctly describe the REINFORCE (vanilla policy gradient) algorithm?",
    options: [
      {
        text: "It estimates gradients using sampled trajectories.",
        isCorrect: true,
      },
      {
        text: "It applies gradient ascent to policy parameters.",
        isCorrect: true,
      },
      { text: "It uses the log-probability of actions.", isCorrect: true },
      { text: "It is an on-policy algorithm.", isCorrect: true },
    ],
    explanation:
      "REINFORCE computes an unbiased gradient estimate using sampled rollouts. Because samples must come from the current policy, it is strictly on-policy.",
  },

  {
    id: "cs224r-lect3-q06",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statements correctly describe the log-derivative trick?",
    options: [
      {
        text: "\\(\\nabla_\\theta p_\\theta(x) = p_\\theta(x) \\nabla_\\theta \\log p_\\theta(x)\\).",
        isCorrect: true,
      },
      {
        text: "It allows gradients to be moved inside an expectation.",
        isCorrect: true,
      },
      {
        text: "It avoids differentiating through the sampling operation.",
        isCorrect: true,
      },
      {
        text: "It is central to policy gradient derivations.",
        isCorrect: true,
      },
    ],
    explanation:
      "The log-derivative trick rewrites gradients of probabilities in a form suitable for Monte Carlo estimation. This is essential because sampling itself is not differentiable.",
  },

  {
    id: "cs224r-lect3-q07",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statements correctly describe the policy gradient theorem?",
    options: [
      {
        text: "\\(\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau}[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)]\\).",
        isCorrect: true,
      },
      {
        text: "Only the policy terms contribute to the gradient.",
        isCorrect: true,
      },
      {
        text: "The gradient does not require knowing environment dynamics.",
        isCorrect: true,
      },
      {
        text: "The expectation can be estimated with Monte Carlo samples.",
        isCorrect: true,
      },
    ],
    explanation:
      "After applying the log trick, gradients of dynamics cancel out. This allows policy gradients to be computed without a model of the environment.",
  },

  {
    id: "cs224r-lect3-q08",
    chapter: 3,
    difficulty: "hard",
    prompt:
      "Which statements correctly describe the gradient \\(\\nabla_\\theta \\log p_\\theta(\\tau)\\)?",
    options: [
      {
        text: "\\(\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\).",
        isCorrect: true,
      },
      {
        text: "Gradients of initial state probabilities vanish.",
        isCorrect: true,
      },
      { text: "Gradients of transition dynamics vanish.", isCorrect: true },
      { text: "The gradient decomposes as a sum over time.", isCorrect: true },
    ],
    explanation:
      "Because neither the initial state distribution nor environment dynamics depend on \\(\\theta\\), their gradients are zero. Only policy log-probabilities remain.",
  },

  {
    id: "cs224r-lect3-q09",
    chapter: 3,
    difficulty: "hard",
    prompt:
      "Which statements correctly describe the intuition of policy gradients?",
    options: [
      {
        text: "Actions in high-reward trajectories become more likely.",
        isCorrect: true,
      },
      {
        text: "Actions in low-reward trajectories become less likely.",
        isCorrect: true,
      },
      {
        text: "The policy imitates its own successful behavior.",
        isCorrect: true,
      },
      {
        text: "Learning corresponds to trial-and-error optimization.",
        isCorrect: true,
      },
    ],
    explanation:
      "Policy gradients increase the probability of actions that led to good outcomes and suppress those that led to poor outcomes. This formalizes trial-and-error learning.",
  },

  // ============================================================
  // Q10–Q18: EXACTLY THREE TRUE
  // ============================================================

  {
    id: "cs224r-lect3-q10",
    chapter: 3,
    difficulty: "easy",
    prompt: "Which statements about policy initialization are correct?",
    options: [
      { text: "Policies can be initialized randomly.", isCorrect: true },
      {
        text: "Policies can be initialized via imitation learning.",
        isCorrect: true,
      },
      { text: "Initialization affects early exploration.", isCorrect: true },
      {
        text: "Initialization determines the optimal final policy.",
        isCorrect: false,
      },
    ],
    explanation:
      "Initialization influences early learning dynamics and exploration but does not uniquely determine the final optimum if learning proceeds correctly.",
  },

  {
    id: "cs224r-lect3-q11",
    chapter: 3,
    difficulty: "medium",
    prompt:
      "Which statements about Monte Carlo estimation of \\(J(\\theta)\\) are correct?",
    options: [
      {
        text: "It approximates expectations using sampled trajectories.",
        isCorrect: true,
      },
      {
        text: "Variance decreases as the number of samples increases.",
        isCorrect: true,
      },
      {
        text: "It requires differentiating through the environment.",
        isCorrect: false,
      },
      {
        text: "It provides an unbiased estimate of expected return.",
        isCorrect: true,
      },
    ],
    explanation:
      "Monte Carlo estimates rely on sampling and averaging. They are unbiased but can suffer from high variance with small sample sizes.",
  },

  {
    id: "cs224r-lect3-q12",
    chapter: 3,
    difficulty: "medium",
    prompt:
      "Which statements about reward weighting in policy gradients are correct?",
    options: [
      { text: "Higher rewards scale gradients upward.", isCorrect: true },
      {
        text: "Negative rewards can reverse gradient direction.",
        isCorrect: true,
      },
      { text: "Reward scaling affects variance.", isCorrect: true },
      {
        text: "Reward scaling does not affect learning dynamics.",
        isCorrect: false,
      },
    ],
    explanation:
      "Rewards directly weight gradient contributions, affecting both direction and variance. Poorly scaled rewards can destabilize learning.",
  },

  {
    id: "cs224r-lect3-q13",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statements about causality in policy gradients are correct?",
    options: [
      { text: "Actions cannot influence past rewards.", isCorrect: true },
      {
        text: "Using future rewards reduces gradient variance.",
        isCorrect: true,
      },
      {
        text: "Returns are often defined as \\(G_t = \\sum_{t'=t}^T r_{t'}\\).",
        isCorrect: true,
      },
      {
        text: "Causality introduces bias into the gradient.",
        isCorrect: false,
      },
    ],
    explanation:
      "Restricting credit assignment to future rewards respects causality and reduces variance without introducing bias.",
  },

  {
    id: "cs224r-lect3-q14",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statements about baselines in policy gradients are correct?",
    options: [
      {
        text: "Baselines reduce variance of gradient estimates.",
        isCorrect: true,
      },
      { text: "Baselines must not depend on the action.", isCorrect: true },
      {
        text: "Subtracting a constant baseline biases the gradient.",
        isCorrect: false,
      },
      { text: "Average reward is a common baseline choice.", isCorrect: true },
    ],
    explanation:
      "A baseline independent of actions preserves unbiasedness while reducing variance. Average reward is a simple and effective choice.",
  },

  {
    id: "cs224r-lect3-q15",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statements about the unbiasedness of baselines are correct?",
    options: [
      {
        text: "\\(\\mathbb{E}[\\nabla_\\theta \\log p_\\theta(\\tau) b] = 0\\).",
        isCorrect: true,
      },
      {
        text: "This follows from \\(\\int p_\\theta(\\tau) d\\tau = 1\\).",
        isCorrect: true,
      },
      { text: "The gradient of a constant is zero.", isCorrect: true },
      { text: "Baselines eliminate all gradient variance.", isCorrect: false },
    ],
    explanation:
      "Subtracting a constant baseline leaves the expected gradient unchanged. While variance is reduced, it is not eliminated entirely.",
  },

  {
    id: "cs224r-lect3-q16",
    chapter: 3,
    difficulty: "easy",
    prompt: "Which statements about reward sparsity are correct?",
    options: [
      { text: "Sparse rewards increase gradient variance.", isCorrect: true },
      { text: "Dense rewards provide more learning signal.", isCorrect: true },
      {
        text: "Policy gradients work best with dense rewards.",
        isCorrect: true,
      },
      {
        text: "Sparse rewards guarantee faster convergence.",
        isCorrect: false,
      },
    ],
    explanation:
      "Sparse rewards provide little feedback, making gradient estimates noisy. Dense rewards improve learning stability.",
  },

  {
    id: "cs224r-lect3-q17",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statements about the surrogate objective are correct?",
    options: [
      {
        text: "It has the same gradient as the true objective.",
        isCorrect: true,
      },
      {
        text: "It enables efficient automatic differentiation.",
        isCorrect: true,
      },
      {
        text: "It avoids computing many separate backward passes.",
        isCorrect: true,
      },
      { text: "It exactly equals the expected return.", isCorrect: false },
    ],
    explanation:
      "The surrogate objective is designed so its gradient matches the policy gradient. It is not equal to the true objective but is computationally convenient.",
  },

  {
    id: "cs224r-lect3-q18",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statements about gradient variance are correct?",
    options: [
      { text: "Variance decreases with larger batch sizes.", isCorrect: true },
      { text: "Variance depends on reward scale.", isCorrect: true },
      {
        text: "Variance is zero for deterministic policies.",
        isCorrect: false,
      },
      { text: "Variance affects learning speed.", isCorrect: true },
    ],
    explanation:
      "High variance slows convergence and destabilizes learning. Larger batches and better baselines reduce variance.",
  },

  // ============================================================
  // Q19–Q27: EXACTLY TWO TRUE
  // ============================================================

  {
    id: "cs224r-lect3-q19",
    chapter: 3,
    difficulty: "easy",
    prompt:
      "Which statements about imitation learning versus policy gradients are correct?",
    options: [
      {
        text: "Policy gradients can outperform demonstrators.",
        isCorrect: true,
      },
      { text: "Imitation learning uses reward signals.", isCorrect: false },
      { text: "Policy gradients require online interaction.", isCorrect: true },
      {
        text: "Imitation learning improves through trial and error.",
        isCorrect: false,
      },
    ],
    explanation:
      "Policy gradients improve by interacting with the environment, while imitation learning simply copies demonstrations.",
  },

  {
    id: "cs224r-lect3-q20",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statements about on-policy algorithms are correct?",
    options: [
      { text: "They require fresh data after each update.", isCorrect: true },
      { text: "They can reuse old data indefinitely.", isCorrect: false },
      { text: "REINFORCE is an on-policy algorithm.", isCorrect: true },
      { text: "They eliminate distribution shift entirely.", isCorrect: false },
    ],
    explanation:
      "On-policy methods rely on samples from the current policy. Old data becomes invalid once the policy changes.",
  },

  {
    id: "cs224r-lect3-q21",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statements about importance sampling are correct?",
    options: [
      {
        text: "It reweights samples from a proposal distribution.",
        isCorrect: true,
      },
      {
        text: "It requires \\(q(x) > 0\\) whenever \\(p(x) > 0\\).",
        isCorrect: true,
      },
      { text: "It always reduces variance.", isCorrect: false },
      { text: "It guarantees stable learning.", isCorrect: false },
    ],
    explanation:
      "Importance sampling corrects for distribution mismatch but can increase variance if weights explode or vanish.",
  },

  {
    id: "cs224r-lect3-q22",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statements about off-policy policy gradients are correct?",
    options: [
      {
        text: "They allow multiple gradient steps per batch.",
        isCorrect: true,
      },
      { text: "They require importance sampling ratios.", isCorrect: true },
      { text: "They are always unbiased in practice.", isCorrect: false },
      { text: "They remove the need for baselines.", isCorrect: false },
    ],
    explanation:
      "Off-policy gradients trade bias and variance to improve sample efficiency. Importance sampling is required to correct for policy mismatch.",
  },

  {
    id: "cs224r-lect3-q23",
    chapter: 3,
    difficulty: "hard",
    prompt:
      "Which statements about trajectory-level importance sampling are correct?",
    options: [
      {
        text: "It involves a product of per-step probability ratios.",
        isCorrect: true,
      },
      { text: "It can explode or vanish for long horizons.", isCorrect: true },
      { text: "It is numerically stable for large \\(T\\).", isCorrect: false },
      { text: "It avoids distribution mismatch entirely.", isCorrect: false },
    ],
    explanation:
      "Multiplying many probability ratios leads to numerical instability. This motivates per-timestep approximations.",
  },

  {
    id: "cs224r-lect3-q24",
    chapter: 3,
    difficulty: "medium",
    prompt:
      "Which statements about per-timestep importance sampling are correct?",
    options: [
      {
        text: "It approximates trajectory-level importance sampling.",
        isCorrect: true,
      },
      {
        text: "It reduces variance compared to full products.",
        isCorrect: true,
      },
      { text: "It is always theoretically exact.", isCorrect: false },
      { text: "It is commonly used in practice.", isCorrect: false },
    ],
    explanation:
      "Per-timestep importance sampling sacrifices theoretical correctness for numerical stability.",
  },

  {
    id: "cs224r-lect3-q25",
    chapter: 3,
    difficulty: "easy",
    prompt: "Which statements about reward scaling are correct?",
    options: [
      { text: "Reward scaling affects gradient magnitude.", isCorrect: true },
      { text: "Reward scaling affects variance.", isCorrect: true },
      { text: "Reward scaling leaves learning unchanged.", isCorrect: false },
      { text: "Reward scaling has no practical impact.", isCorrect: false },
    ],
    explanation:
      "The scale of rewards directly impacts gradient updates and stability.",
  },

  {
    id: "cs224r-lect3-q26",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statements about gradient estimation noise are correct?",
    options: [
      { text: "Noise arises from stochastic policies.", isCorrect: true },
      { text: "Noise arises from stochastic environments.", isCorrect: true },
      { text: "Noise disappears with baselines.", isCorrect: false },
      { text: "Noise makes convergence slower.", isCorrect: false },
    ],
    explanation:
      "Multiple sources contribute to gradient noise. Baselines reduce but do not eliminate it.",
  },

  {
    id: "cs224r-lect3-q27",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statements about returns \\(G_t\\) are correct?",
    options: [
      {
        text: "\\(G_t\\) sums rewards from time \\(t\\) onward.",
        isCorrect: true,
      },
      { text: "\\(G_t\\) respects causality.", isCorrect: true },
      { text: "\\(G_t\\) includes past rewards.", isCorrect: false },
      { text: "\\(G_t\\) guarantees low variance.", isCorrect: false },
    ],
    explanation:
      "Returns focus credit assignment on future rewards, aligning with causal influence.",
  },

  // ============================================================
  // Q28–Q35: EXACTLY ONE TRUE
  // ============================================================

  {
    id: "cs224r-lect3-q28",
    chapter: 3,
    difficulty: "easy",
    prompt: "Which statement about REINFORCE is correct?",
    options: [
      { text: "It is an off-policy algorithm.", isCorrect: false },
      { text: "It reuses data from old policies.", isCorrect: false },
      {
        text: "It estimates gradients from on-policy rollouts.",
        isCorrect: true,
      },
      { text: "It requires a value function.", isCorrect: false },
    ],
    explanation:
      "REINFORCE relies exclusively on samples from the current policy.",
  },

  {
    id: "cs224r-lect3-q29",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statement about baselines is correct?",
    options: [
      { text: "They bias the gradient.", isCorrect: false },
      { text: "They change the optimal policy.", isCorrect: false },
      {
        text: "They reduce variance without changing expectation.",
        isCorrect: true,
      },
      { text: "They eliminate stochasticity.", isCorrect: false },
    ],
    explanation:
      "Baselines are designed to reduce variance while keeping the expected gradient unchanged.",
  },

  {
    id: "cs224r-lect3-q30",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statement about importance sampling ratios is correct?",
    options: [
      { text: "They are always close to 1.", isCorrect: false },
      {
        text: "They are unnecessary in off-policy learning.",
        isCorrect: false,
      },
      { text: "They correct for policy mismatch.", isCorrect: true },
      { text: "They eliminate distribution shift.", isCorrect: false },
    ],
    explanation:
      "Importance sampling ratios reweight samples to account for differences between old and new policies.",
  },

  {
    id: "cs224r-lect3-q31",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statement about policy gradient variance is correct?",
    options: [
      {
        text: "Variance decreases with smaller batch sizes.",
        isCorrect: false,
      },
      { text: "Variance is unaffected by reward design.", isCorrect: false },
      { text: "Variance is a major practical challenge.", isCorrect: true },
      {
        text: "Variance disappears in continuous action spaces.",
        isCorrect: false,
      },
    ],
    explanation:
      "High variance is a central challenge in policy gradient methods and motivates variance-reduction techniques.",
  },

  {
    id: "cs224r-lect3-q32",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statement about off-policy policy gradients is correct?",
    options: [
      {
        text: "They are always more stable than on-policy methods.",
        isCorrect: false,
      },
      { text: "They avoid importance sampling.", isCorrect: false },
      { text: "They allow more gradient updates per batch.", isCorrect: true },
      { text: "They require no assumptions.", isCorrect: false },
    ],
    explanation:
      "Off-policy methods improve sample efficiency but introduce new stability challenges.",
  },

  {
    id: "cs224r-lect3-q33",
    chapter: 3,
    difficulty: "easy",
    prompt:
      "Which statement best summarizes the intuition of policy gradients?",
    options: [
      { text: "Copy expert demonstrations.", isCorrect: false },
      { text: "Predict rewards directly.", isCorrect: false },
      {
        text: "Do more high-reward actions and less low-reward actions.",
        isCorrect: true,
      },
      { text: "Optimize environment dynamics.", isCorrect: false },
    ],
    explanation:
      "Policy gradients reinforce actions that lead to good outcomes and suppress poor ones.",
  },

  {
    id: "cs224r-lect3-q34",
    chapter: 3,
    difficulty: "medium",
    prompt: "Which statement about gradient estimation is correct?",
    options: [
      {
        text: "Exact gradients can be computed analytically.",
        isCorrect: false,
      },
      {
        text: "Gradients require differentiating through the environment.",
        isCorrect: false,
      },
      {
        text: "Gradients are estimated using sampled trajectories.",
        isCorrect: true,
      },
      { text: "Gradients are deterministic.", isCorrect: false },
    ],
    explanation:
      "Policy gradients rely on Monte Carlo estimates from sampled rollouts.",
  },

  {
    id: "cs224r-lect3-q35",
    chapter: 3,
    difficulty: "hard",
    prompt: "Which statement about policy gradient methods is correct?",
    options: [
      { text: "They are low-variance by default.", isCorrect: false },
      { text: "They eliminate the need for exploration.", isCorrect: false },
      {
        text: "They form the foundation of actor-critic methods.",
        isCorrect: true,
      },
      { text: "They are unsuitable for continuous control.", isCorrect: false },
    ],
    explanation:
      "Actor-critic methods build directly on policy gradient ideas while addressing variance and sample efficiency.",
  },
];
