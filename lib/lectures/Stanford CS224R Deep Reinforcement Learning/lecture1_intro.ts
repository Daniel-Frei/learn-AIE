import { Question } from "../../quiz";

export const cs224rLecture1IntroQuestions: Question[] = [
  // ============================================================
  // CS224R – Lecture 1: Introduction to Deep Reinforcement Learning
  // Q1–Q35
  // ============================================================

  // ============================================================
  // Q1–Q9: ALL TRUE
  // ============================================================

  {
    id: "cs224r-lect1-q01",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements correctly describe reinforcement learning problems?",
    options: [
      { text: "They involve sequential decision-making over time.", isCorrect: true },
      { text: "Actions influence future observations and outcomes.", isCorrect: true },
      { text: "Feedback is often indirect rather than labeled targets.", isCorrect: true },
      { text: "Learning typically occurs from experience collected by the agent.", isCorrect: true },
    ],
    explanation:
      "Reinforcement learning focuses on agents interacting with environments over time. Unlike supervised learning, the agent is not told the correct action directly and must infer good behavior from delayed and indirect feedback."
  },

  {
    id: "cs224r-lect1-q02",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements correctly characterize a policy in reinforcement learning?",
    options: [
      { text: "A policy maps states or observations to actions.", isCorrect: true },
      { text: "Policies are often denoted by the symbol π (pi).", isCorrect: true },
      { text: "A policy may be represented by a neural network.", isCorrect: true },
      { text: "Policies can be stochastic rather than deterministic.", isCorrect: true },
    ],
    explanation:
      "A policy defines the agent’s behavior. In deep reinforcement learning, policies are commonly parameterized by neural networks and may output probability distributions over actions."
  },

  {
    id: "cs224r-lect1-q03",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements correctly describe a trajectory in reinforcement learning?",
    options: [
      { text: "It is a sequence of states or observations and actions.", isCorrect: true },
      { text: "It may include rewards associated with each step.", isCorrect: true },
      { text: "It is generated by rolling out a policy in an environment.", isCorrect: true },
      { text: "It can have variable length depending on the task.", isCorrect: true },
    ],
    explanation:
      "A trajectory captures one interaction episode between an agent and the environment. Its length may be fixed or variable depending on termination conditions and task structure."
  },

  {
    id: "cs224r-lect1-q04",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements correctly describe the Markov property?",
    options: [
      { text: "The next state depends only on the current state and action.", isCorrect: true },
      { text: "Past states become irrelevant when conditioning on the current state.", isCorrect: true },
      { text: "It simplifies modeling long sequences of decisions.", isCorrect: true },
      { text: "It underlies the definition of a Markov Decision Process.", isCorrect: true },
    ],
    explanation:
      "The Markov property allows future predictions to ignore distant history once the current state is known. This assumption is central to most reinforcement learning formulations."
  },

  {
    id: "cs224r-lect1-q05",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements correctly describe observations in reinforcement learning?",
    options: [
      { text: "Observations may not contain all information about the true state.", isCorrect: true },
      { text: "They are what the agent directly perceives.", isCorrect: true },
      { text: "They can require memory of past observations to act optimally.", isCorrect: true },
      { text: "They are commonly used in partially observable environments.", isCorrect: true },
    ],
    explanation:
      "When the agent cannot observe the full state, it must rely on observations. Memory or history helps compensate for missing information."
  },

  {
    id: "cs224r-lect1-q06",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements correctly describe reward functions?",
    options: [
      { text: "They define how desirable a state or action is.", isCorrect: true },
      { text: "They encode the task objective.", isCorrect: true },
      { text: "They can depend on the state, the action, or both.", isCorrect: true },
      { text: "They guide learning without specifying correct actions explicitly.", isCorrect: true },
    ],
    explanation:
      "Rewards specify what the agent should achieve, not how to achieve it. This indirect supervision is a key distinction from supervised learning."
  },

  {
    id: "cs224r-lect1-q07",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements correctly describe the reinforcement learning objective?",
    options: [
      { text: "It maximizes expected cumulative reward.", isCorrect: true },
      { text: "The expectation is taken over trajectories induced by the policy.", isCorrect: true },
      { text: "Stochasticity in the environment affects the objective.", isCorrect: true },
      { text: "Policy parameters influence the objective indirectly through trajectory distributions.", isCorrect: true },
    ],
    explanation:
      "The policy affects which trajectories are likely, and rewards are accumulated along those trajectories. Optimization therefore occurs over expected outcomes rather than fixed labels."
  },

  {
    id: "cs224r-lect1-q08",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements correctly describe stochastic policies?",
    options: [
      { text: "They assign probabilities to actions.", isCorrect: true },
      { text: "They support exploration during learning.", isCorrect: true },
      { text: "They can model variability in demonstrated behavior.", isCorrect: true },
      { text: "They are common in modern reinforcement learning methods.", isCorrect: true },
    ],
    explanation:
      "Stochastic policies enable trying multiple behaviors and capturing uncertainty. They are especially important when learning from experience or demonstrations."
  },

  {
    id: "cs224r-lect1-q09",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements correctly describe value functions?",
    options: [
      { text: "They estimate future expected reward.", isCorrect: true },
      { text: "They depend on a specific policy.", isCorrect: true },
      { text: "They help evaluate how good a state or action is.", isCorrect: true },
      { text: "They are central to many reinforcement learning algorithms.", isCorrect: true },
    ],
    explanation:
      "Value functions quantify how promising a situation is under a given policy. They are used to guide policy improvement in many algorithms."
  },

  // ============================================================
  // Q10–Q18: EXACTLY 3 TRUE
  // ============================================================

  {
    id: "cs224r-lect1-q10",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements about supervised learning versus reinforcement learning are correct?",
    options: [
      { text: "Supervised learning typically uses labeled input–output pairs.", isCorrect: true },
      { text: "Reinforcement learning feedback is often delayed.", isCorrect: true },
      { text: "Reinforcement learning data is not independent and identically distributed.", isCorrect: true },
      { text: "Supervised learning requires interaction with an environment.", isCorrect: false },
    ],
    explanation:
      "Supervised learning relies on fixed labeled datasets, while reinforcement learning depends on interaction. Actions in reinforcement learning influence future data, breaking the independent and identically distributed assumption."
  },

  {
    id: "cs224r-lect1-q11",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about Markov Decision Processes are correct?",
    options: [
      { text: "They consist of states, actions, rewards, and dynamics.", isCorrect: true },
      { text: "They assume the Markov property holds.", isCorrect: true },
      { text: "They define a probability distribution over trajectories.", isCorrect: true },
      { text: "They require full observability of the environment.", isCorrect: false },
    ],
    explanation:
      "A Markov Decision Process assumes full access to the state, but this assumption can be relaxed in partially observable settings."
  },

  {
    id: "cs224r-lect1-q12",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about observations are correct?",
    options: [
      { text: "They may require maintaining a history to infer the state.", isCorrect: true },
      { text: "They are always equivalent to the true state.", isCorrect: false },
      { text: "They can come from sensors or user inputs.", isCorrect: true },
      { text: "They can lead to partially observable problems.", isCorrect: true },
    ],
    explanation:
      "Observations may omit important information. Using history or memory helps compensate for this partial observability."
  },

  {
    id: "cs224r-lect1-q13",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about trajectories are correct?",
    options: [
      { text: "They depend on both the policy and environment dynamics.", isCorrect: true },
      { text: "They are deterministic given a policy.", isCorrect: false },
      { text: "They can be used as training data.", isCorrect: true },
      { text: "They represent experience collected by the agent.", isCorrect: true },
    ],
    explanation:
      "Trajectories capture interaction experience. Randomness in the environment or policy means trajectories vary even under the same policy."
  },

  {
    id: "cs224r-lect1-q14",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements about stochasticity in reinforcement learning are correct?",
    options: [
      { text: "Environment transitions can be stochastic.", isCorrect: true },
      { text: "Policies can introduce randomness in action selection.", isCorrect: true },
      { text: "Stochasticity affects expected returns.", isCorrect: true },
      { text: "Stochasticity makes learning impossible.", isCorrect: false },
    ],
    explanation:
      "Stochasticity introduces uncertainty but does not prevent learning. Algorithms explicitly account for randomness through expectations."
  },

  {
    id: "cs224r-lect1-q15",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements about discount factors are correct?",
    options: [
      { text: "They weight future rewards relative to immediate rewards.", isCorrect: true },
      { text: "A discount factor of 1 treats all rewards equally.", isCorrect: true },
      { text: "Smaller discount factors emphasize near-term rewards.", isCorrect: true },
      { text: "Discount factors eliminate the need for value functions.", isCorrect: false },
    ],
    explanation:
      "Discounting controls the importance of future outcomes. Value functions remain necessary to evaluate long-term consequences."
  },

  {
    id: "cs224r-lect1-q16",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about policies parameterized by neural networks are correct?",
    options: [
      { text: "They can process high-dimensional inputs like images.", isCorrect: true },
      { text: "They can be trained using gradient-based methods.", isCorrect: true },
      { text: "They require labeled action targets.", isCorrect: false },
      { text: "They are common in deep reinforcement learning.", isCorrect: true },
    ],
    explanation:
      "Deep reinforcement learning uses neural networks to handle complex inputs. Learning is driven by rewards rather than explicit action labels."
  },

  {
    id: "cs224r-lect1-q17",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements about value functions are correct?",
    options: [
      { text: "They estimate expected future reward under a policy.", isCorrect: true },
      { text: "They depend on the dynamics of the environment.", isCorrect: true },
      { text: "They can be used to improve policies.", isCorrect: true },
      { text: "They eliminate the need for exploration.", isCorrect: false },
    ],
    explanation:
      "Value functions help evaluate and improve policies but do not remove the need to explore uncertain actions."
  },

  {
    id: "cs224r-lect1-q18",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements about reinforcement learning applications are correct?",
    options: [
      { text: "They include robotics and control tasks.", isCorrect: true },
      { text: "They include game playing.", isCorrect: true },
      { text: "They include language model post-training.", isCorrect: true },
      { text: "They are limited to simulated environments.", isCorrect: false },
    ],
    explanation:
      "Reinforcement learning is applied across many domains, including real-world systems. Simulation is common but not mandatory."
  },

  // ============================================================
  // Q19–Q27: EXACTLY 2 TRUE
  // ============================================================

  {
    id: "cs224r-lect1-q19",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements correctly describe actions in reinforcement learning?",
    options: [
      { text: "They are decisions made by the agent.", isCorrect: true },
      { text: "They influence future states.", isCorrect: true },
      { text: "They are always discrete.", isCorrect: false },
      { text: "They are independent of the policy.", isCorrect: false },
    ],
    explanation:
      "Actions represent the agent’s choices and affect how the environment evolves. They may be continuous or discrete and are selected according to the policy."
  },

  {
    id: "cs224r-lect1-q20",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about partially observable Markov decision processes are correct?",
    options: [
      { text: "They arise when the agent cannot observe the full state.", isCorrect: true },
      { text: "They often require memory or belief states.", isCorrect: true },
      { text: "They assume the Markov property over observations.", isCorrect: false },
      { text: "They remove the need for policies.", isCorrect: false },
    ],
    explanation:
      "Partial observability complicates decision-making. Memory or inference over hidden states helps address missing information."
  },

  {
    id: "cs224r-lect1-q21",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about trajectories are correct?",
    options: [
      { text: "They can include both states and actions.", isCorrect: true },
      { text: "They always have fixed length.", isCorrect: false },
      { text: "They are equivalent to supervised datasets.", isCorrect: false },
      { text: "They represent sequences over time.", isCorrect: true },
    ],
    explanation:
      "Trajectories capture temporal structure, which distinguishes them from static datasets. Length can vary depending on termination conditions."
  },

  {
    id: "cs224r-lect1-q22",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements about reward design are correct?",
    options: [
      { text: "Sparse rewards can make learning difficult.", isCorrect: true },
      { text: "Reward shaping can help guide learning.", isCorrect: true },
      { text: "Rewards must always be differentiable.", isCorrect: false },
      { text: "Rewards uniquely determine optimal actions.", isCorrect: false },
    ],
    explanation:
      "Sparse rewards provide little feedback, slowing learning. Rewards need not be differentiable and often allow multiple optimal behaviors."
  },

  {
    id: "cs224r-lect1-q23",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements about expected return are correct?",
    options: [
      { text: "It averages rewards over possible trajectories.", isCorrect: true },
      { text: "It depends on the policy.", isCorrect: true },
      { text: "It ignores stochasticity.", isCorrect: false },
      { text: "It is identical to immediate reward.", isCorrect: false },
    ],
    explanation:
      "Expected return accounts for uncertainty by averaging over outcomes. It captures long-term performance rather than immediate reward."
  },

  {
    id: "cs224r-lect1-q24",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statements about reinforcement learning data are correct?",
    options: [
      { text: "It is generated through interaction.", isCorrect: true },
      { text: "It is independent and identically distributed.", isCorrect: false },
      { text: "It reflects the agent’s current policy.", isCorrect: true },
      { text: "It never changes over time.", isCorrect: false },
    ],
    explanation:
      "Data in reinforcement learning depends on the policy and evolves as the policy changes. This violates the independent and identically distributed assumption."
  },

  {
    id: "cs224r-lect1-q25",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about neural network policies are correct?",
    options: [
      { text: "They can consume observations or states as input.", isCorrect: true },
      { text: "They require full state observability.", isCorrect: false },
      { text: "They can output distributions over actions.", isCorrect: true },
      { text: "They cannot handle high-dimensional inputs.", isCorrect: false },
    ],
    explanation:
      "Neural networks are flexible function approximators. They are widely used to handle high-dimensional sensory inputs."
  },

  {
    id: "cs224r-lect1-q26",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statements about the probability of a trajectory are correct?",
    options: [
      { text: "It factors into initial state, policy, and dynamics terms.", isCorrect: true },
      { text: "It depends on the policy parameters.", isCorrect: true },
      { text: "It is independent of the environment.", isCorrect: false },
      { text: "It ignores rewards.", isCorrect: false },
    ],
    explanation:
      "Trajectory probabilities combine environment dynamics and policy behavior. Rewards are not part of the probability but affect the objective."
  },

  {
    id: "cs224r-lect1-q27",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statements about exploration are correct?",
    options: [
      { text: "It is necessary to discover good actions.", isCorrect: true },
      { text: "It can be enabled by stochastic policies.", isCorrect: true },
      { text: "It guarantees optimal behavior.", isCorrect: false },
      { text: "It is unnecessary once learning begins.", isCorrect: false },
    ],
    explanation:
      "Exploration allows agents to gather informative experience. It does not guarantee optimality but is essential for learning."
  },

  // ============================================================
  // Q28–Q35: EXACTLY 1 TRUE
  // ============================================================

  {
    id: "cs224r-lect1-q28",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statement best describes the goal of reinforcement learning?",
    options: [
      { text: "Maximizing expected cumulative reward.", isCorrect: true },
      { text: "Predicting labels from fixed datasets.", isCorrect: false },
      { text: "Minimizing classification error.", isCorrect: false },
      { text: "Matching human demonstrations exactly.", isCorrect: false },
    ],
    explanation:
      "Reinforcement learning optimizes long-term reward rather than immediate accuracy or label matching."
  },

  {
    id: "cs224r-lect1-q29",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statement correctly describes a value function?",
    options: [
      { text: "It estimates expected future reward under a policy.", isCorrect: true },
      { text: "It specifies which action to take.", isCorrect: false },
      { text: "It defines environment dynamics.", isCorrect: false },
      { text: "It replaces the reward function.", isCorrect: false },
    ],
    explanation:
      "Value functions evaluate states or actions but do not directly select actions."
  },

  {
    id: "cs224r-lect1-q30",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statement correctly describes why reinforcement learning data is non-independent and identically distributed?",
    options: [
      { text: "Actions influence future observations.", isCorrect: true },
      { text: "Rewards are deterministic.", isCorrect: false },
      { text: "Policies are always fixed.", isCorrect: false },
      { text: "States are randomly shuffled.", isCorrect: false },
    ],
    explanation:
      "Because actions affect future states, data depends on past decisions and policy behavior."
  },

  {
    id: "cs224r-lect1-q31",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statement correctly describes a partially observable environment?",
    options: [
      { text: "The agent cannot directly observe the full state.", isCorrect: true },
      { text: "The environment is deterministic.", isCorrect: false },
      { text: "Rewards are unavailable.", isCorrect: false },
      { text: "Policies are unnecessary.", isCorrect: false },
    ],
    explanation:
      "Partial observability arises when sensors or inputs provide incomplete information about the world."
  },

  {
    id: "cs224r-lect1-q32",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statement correctly explains why stochastic policies are useful?",
    options: [
      { text: "They encourage exploration during learning.", isCorrect: true },
      { text: "They remove uncertainty from the environment.", isCorrect: false },
      { text: "They guarantee optimal actions.", isCorrect: false },
      { text: "They eliminate the need for rewards.", isCorrect: false },
    ],
    explanation:
      "Stochastic policies allow agents to try different actions and gather informative experience."
  },

  {
    id: "cs224r-lect1-q33",
    chapter: 1,
    difficulty: "easy",
    prompt: "Which statement correctly describes an action in reinforcement learning?",
    options: [
      { text: "It is a decision chosen by the agent.", isCorrect: true },
      { text: "It is the environment’s response.", isCorrect: false },
      { text: "It is identical to a reward.", isCorrect: false },
      { text: "It represents the state of the world.", isCorrect: false },
    ],
    explanation:
      "Actions are chosen by the agent and influence how the environment evolves."
  },

  {
    id: "cs224r-lect1-q34",
    chapter: 1,
    difficulty: "medium",
    prompt: "Which statement correctly describes discounting in reinforcement learning?",
    options: [
      { text: "It controls the importance of future rewards.", isCorrect: true },
      { text: "It removes randomness from rewards.", isCorrect: false },
      { text: "It guarantees convergence.", isCorrect: false },
      { text: "It replaces value functions.", isCorrect: false },
    ],
    explanation:
      "Discounting reflects how much the agent values future outcomes relative to immediate ones."
  },

  {
    id: "cs224r-lect1-q35",
    chapter: 1,
    difficulty: "hard",
    prompt: "Which statement correctly characterizes a Markov Decision Process?",
    options: [
      { text: "Future states depend only on the current state and action.", isCorrect: true },
      { text: "Observations always equal states.", isCorrect: false },
      { text: "Rewards must be deterministic.", isCorrect: false },
      { text: "Policies cannot be stochastic.", isCorrect: false },
    ],
    explanation:
      "The defining feature of a Markov Decision Process is the Markov property. Other assumptions such as determinism are not required."
  },
];
